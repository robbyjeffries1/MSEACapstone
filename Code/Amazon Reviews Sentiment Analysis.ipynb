{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55021a7c",
   "metadata": {},
   "source": [
    "# Amazon Reviews Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894f9829",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0a0fe6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Importing the Data\n",
    "%cd ..\n",
    "df = pd.read_csv('/Users/robbyjeffries/MSEACapstone/Data/CSV_cleaned/Grocery_and_Gourmet_Food.csv')\n",
    "df = df[['reviewText', 'overall']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6361cd5",
   "metadata": {},
   "source": [
    "Tentatively, I will classify reviews that scored 3 and above as positive and the rest as negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a9385d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Sentiment Column\n",
    "df['sentiment'] = df['overall'].apply(lambda x: 1 if x>=3 else 0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07c7421",
   "metadata": {},
   "source": [
    "### Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff84111",
   "metadata": {},
   "source": [
    "Before we start building our model, we need to get rid of special characters, duplicates characters and standardize the formatting of the reviews. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83dd1a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "\n",
    "if False: # Change it to true if you haven't installed it\n",
    "    !pip install text-preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1217d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_preprocessing import preprocess_text\n",
    "from text_preprocessing import to_lower, remove_email, remove_url, remove_punctuation, lemmatize_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924dd36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_process = df['reviewText']\n",
    "\n",
    "text_to_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0ba12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_process = df['reviewText']\n",
    "\n",
    "# Preprocess text using custom preprocess functions in the pipeline \n",
    "preprocess_functions = [to_lower, remove_email, remove_url, remove_punctuation, lemmatize_word]\n",
    "preprocessed_text = preprocess_text(text_to_process, preprocess_functions)\n",
    "print(preprocessed_text)\n",
    "# output: helllo i am john doe my email is visit our website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "57326a87",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/c9/z8kjq8qs15n6nq0dr7vdg5640000gn/T/ipykernel_45382/2391040083.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtext_preprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpreprocess_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mudf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStringType\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataFrame\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mSparkDataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "from text_preprocessing import preprocess_text\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql import DataFrame as SparkDataFrame\n",
    "\n",
    "\n",
    "def preprocess_text_spark(df: SparkDataFrame, \n",
    "                          target_column: str, \n",
    "                          preprocessed_column_name: str = 'preprocessed_text'\n",
    "                         ) -> SparkDataFrame:\n",
    "    \"\"\" Preprocess text in a column of a PySpark DataFrame by leveraging PySpark UDF to preprocess text in parallel \"\"\"\n",
    "    _preprocess_text = udf(preprocess_text, StringType())\n",
    "    new_df = df.withColumn(preprocessed_column_name, _preprocess_text(df[target_column]))\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246731d3",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640580af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import preprocess_kgptalkie as ps\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b85b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clean(x):\n",
    "    '''\n",
    "    Reformat a string by removing duplicates, and special characters\n",
    "    e.g: you're -> you are\n",
    "         i'm -> i am\n",
    "         I llllovvee iit!! -> i love it\n",
    "         white_dog -> white dog\n",
    "    '''\n",
    "    x = str(x).lower().replace('\\\\', '').replace('_', ' ')\n",
    "    x = ps.cont_exp(x) # you're -> you are; i'm -> i am\n",
    "    x = ps.remove_emails(x)\n",
    "    x = ps.remove_urls(x)\n",
    "    x = ps.remove_html_tags(x)\n",
    "    x = ps.remove_accented_chars(x)\n",
    "    x = ps.remove_special_chars(x)\n",
    "    x = re.sub(\"(.)\\\\1{2,}\", \"\\\\1\", x) # (e.g: I llllovvee iit -> I love it)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4262282",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['reviewText'] = df['reviewText'].apply(lambda x: get_clean(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132f3e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62472a57",
   "metadata": {},
   "source": [
    "### Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c4eb56",
   "metadata": {},
   "source": [
    "We will use a Support Vector Machine\n",
    "\n",
    "A support vector machine (SVM) is a supervised machine learning model that uses classification algorithms for two-group classification problems. After giving an SVM model sets of labeled training data for each category, they’re able to categorize new text.\n",
    "\n",
    "The basics of Support Vector Machines and how it works are best understood with a simple example. Let’s imagine we have two tags: red and yellow, and our data has two features: x and y. We want a classifier that, given a pair of (x,y) coordinates, outputs if it’s either red or yellow. We plot our already labeled training data on a plane:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d17c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "X, y = make_blobs(n_samples=50, centers=2,\n",
    "                  random_state=0, cluster_std=0.60)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
    "\n",
    "plt.plot([0.6], [2.5], 'x', color='black', markeredgewidth=2, markersize=10) # Unlabelled Point;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afa3919",
   "metadata": {},
   "source": [
    "A support vector machine takes these data points and outputs the hyperplane (which in two dimensions it’s simply a line) that best separates the tags. This line is the decision boundary: anything that falls to one side of it we will classify as red, and anything that falls to the other as yellow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d81b7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "xfit = np.linspace(-1, 3.5)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
    "plt.plot([0.6], [2.5], 'x', color='red', markeredgewidth=2, markersize=10) # Unlabelled Point\n",
    "\n",
    "\n",
    "plt.plot(xfit, 0.17 * xfit + 2.2, '-k')\n",
    "\n",
    "plt.xlim(-1, 3.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f321f66",
   "metadata": {},
   "source": [
    "Now, we want to apply this algorithm for text classification, and the first thing we need is a way to transform a piece of text into a vector of numbers so we can run SVM with them. In other words, which features do we have to use in order to classify texts using SVM?\n",
    "\n",
    "The most common answer is word frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19a2ad9",
   "metadata": {},
   "source": [
    "**TF-IDF (term frequency-inverse document frequency)** is a statistical measure that evaluates how relevant a word is to a document in a collection of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c23bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2478c7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing Raw text reviews\n",
    "tfidf = TfidfVectorizer(max_features=5000)\n",
    "X=df['review_body']\n",
    "y=df['sentiment']\n",
    "# y=df['star_rating']\n",
    "\n",
    "X = tfidf.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5f29ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b736dcc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X[:2,]) # Text Reviews got recoded in numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168a70b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample, seed\n",
    "\n",
    "seed(2022)\n",
    "# Random Sample of features \n",
    "sample(tfidf.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d84d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partion Data into Train and Test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb0c18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "clf = LinearSVC(loss='hinge') # tweek parametters here to make it better (or worse)\n",
    "\n",
    "# Training Model\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66438d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing Model\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba48930",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162842d4",
   "metadata": {},
   "source": [
    "### Testing the model on few reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75799c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's understand how the algorith works\n",
    "x = ['I love this phone, will definitely recommend',\n",
    "     'This PHONE is terrible, I want a refund!', \n",
    "     'I believe this phone fulfill his purpose, but it could have been better']\n",
    "\n",
    "for i in range(len(x)):\n",
    "       x[i] = get_clean(x[i])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84867dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = tfidf.transform(x) # tokenizing using previously created features\n",
    "vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95d06ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.predict(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d950cf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting the model\n",
    "import pickle\n",
    "\n",
    "\n",
    "pickle.dump(clf, open('Model/amazon_svc', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c1f5ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
